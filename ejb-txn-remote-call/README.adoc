include::../shared-doc/attributes.adoc[]

= ejb-txn-remote-call: Demonstrates remote EJB calls and transaction propagation
:author: Ondra Chaloupka
:level: Intermediate
:technologies: EJB, JTA, Clustering
:openshift: true

[abstract]
The `ejb-txn-remote-call` quickstart demonstrates remote transactional EJB calls over two application servers of {productName}.

:standalone-server-type: ha
:archiveType: war
:requires-multiple-servers:
:jbds-not-supported:

== What is it?

The `ejb-txn-remote-call` quickstart demonstrates the remote transactional EJB calls over two {productNameFull}s. The remote side forms a HA cluster.

== Description

This quickstart demonstrates how EJB remote calls propagate JTA transaction across {productNameFull}s. Further, this quickstart demonstrates the transaction recovery, which is run for both servers when a failure occurs.

This quickstart contains two Maven projects.
The first maven project represents the sender side, and is intended to be deployed on the first {productName} (`server1`).
The second project represents the receiver side. This project is intended to be deployed
to the other two {productName} (`server2` and `server3`). The two projects must not be deployed to the same server.

.Maven projects in this quickstart
[cols="40%,60%",options="headers"]
|===
|Project |Description

|`client`
|The application deployed to the first {productName} server.
Users can interact with this application through some REST endpoints, which start remote EJB calls toward the `server` application
deployed on the other two {productName}s.
In more details, the transaction initiated at the client side enlists two participants: a database, and the EJB remote server.
The transaction manager then uses the two-phase commit to commit the transactions over the two servers.
Moreover, this quickstart shows how transactional failures how dealt with.

|`server`
|The application deployed to the second and third {productName} servers.
This application receives the remote EJB calls from the `client` application and,
depending on the scenario, process the propagated transaction.
In more details, the transaction initiated at the server side enlists two participants: a database, and a mock XAResource.
|===

== Running the Quickstart

This quickstart demonstrates its functionalities on <<_running_in_a_bare_metal_environment, bare metal>>, using {productName} Maven plugin, and on OpenShift.

// System Requirements
include::../shared-doc/system-requirements.adoc[leveloffset=+1]
// Use of {jbossHomeName}_1 and {jbossHomeName}_2
include::../shared-doc/use-of-jboss-home-name.adoc[leveloffset=+1]

// Run the Quickstart in Red Hat CodeReady Studio or Eclipse
include::../shared-doc/run-the-quickstart-in-jboss-developer-studio.adoc[leveloffset=+1]

== The Goal

The EJB remote call propagates transaction from `client` application
to `server` application. The remote call hits one of the two servers where the `server` application is deployed.

== Running in a bare metal environment

First of all, three {productName} servers needs to be configured. Then, the `client` application gets deployed to the first server (`server1`),
while the `server` application gets deployed to the other two {productName} servers (`server2`, and `server3`, which are configured as a cluster).

[[_setup_productname_servers]]
=== Setup {productName} servers

The easiest way to start multiple instances of {productName} on a local computer is to copy the {productName} installation directory
to three separate directories.

The installation directories are named:

* `__{jbossHomeName}_1__` for `server1`
* `__{jbossHomeName}_2__` for `server2`
* `__{jbossHomeName}_3__` for `server3`

Given that the installation directory of the {productName} is identified with ${jbossHomeName}:
[source,sh,subs="+quotes,attributes+"]
----
cp -r ${jbossHomeName} server1; \
{jbossHomeName}_1="$PWD/server1"
----
[source,sh,subs="+quotes,attributes+"]
----
cp -r ${jbossHomeName} server2; \
{jbossHomeName}_2="$PWD/server2"
----
[source,sh,subs="+quotes,attributes+"]
----
cp -r ${jbossHomeName} server3; \
{jbossHomeName}_3="$PWD/server3"
----

=== Creating a user for `server2` and `server3`

To successfully process EJB remote calls from `server1` to either `server2`
or to `server3` a user to authenticate the EJB remote calls must be created on the receiving servers.

Run the following procedure in the directories `__{jbossHomeName}_2__` and `__{jbossHomeName}_3__` to create the user for `server2` and `server3`.

[#add_application_user]
// Add the Authorized Application User
include::../shared-doc/add-application-user.adoc[leveloffset=+3]

[NOTE]
====
For the `add-user.sh` (or `.bat`) command you, can add the parameter `-ds`.
When you include this parameter, after the user is added, the system outputs a secret value that you can use to set up the remote output connection on `server1`.

The output of command when `-ds` parameter is used:

[code,sh]
----
To represent the user add the following to the server-identities definition <secret value="cXVpY2tzdGFydFB3ZDEh" />
----
====

=== Configure datasources

As this quickstart performs transactional work against a database, it is needed to create a new database.
For the purpose of this quickstart, a simple PostgreSQL container will be used:

[source,sh]
----
podman run -p 5432:5432 --rm  -ePOSTGRES_DB=test -ePOSTGRES_USER=test -ePOSTGRES_PASSWORD=test postgres:9.4 -c max-prepared-transactions=110 -c log-statement=all
----

The {productName} servers need to be configured to be able to connect to the database.
First of all, a JDBC driver needs to be installed as https://docs.wildfly.org/30/Developer_Guide.html#Class_Loading_in_WildFly[jboss module].

The following command (along packaging the `client` and the `server` applications) downloads the PostgreSQL driver automatically through Maven:
[source,sh,subs="+quotes,attributes+"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call;
mvn clean package
----
Then, the PostgreSQL driver needs to be loaded as jboss module in all {productName} servers:

[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_1; \
./bin/jboss-cli.sh "embed-server,\
  module add --name=org.postgresql.jdbc \
  --resources=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/target/postgresql/postgresql.jar"
----
[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_2; \
./bin/jboss-cli.sh "embed-server,\
  module add --name=org.postgresql.jdbc \
  --resources=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/target/postgresql/postgresql.jar"
----
[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_3; \
./bin/jboss-cli.sh "embed-server,\
  module add --name=org.postgresql.jdbc \
  --resources=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/target/postgresql/postgresql.jar"
----

Moreover, the PostgreSQL driver needs to be installed on all {productName} servers.
For `server1`, the configuration file `standalone.xml` will be used.
For `server2` and `server3` the configuration file `standalone-ha.xml` will be used.

[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_1; \
./bin/jboss-cli.sh "embed-server --server-config=standalone.xml,\
  /subsystem=datasources/jdbc-driver=postgresql:add(driver-name=postgresql,driver-module-name=org.postgresql.jdbc,driver-xa-datasource-class-name=org.postgresql.xa.PGXADataSource)"
----
[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_2; \
./bin/jboss-cli.sh "embed-server --server-config=standalone-ha.xml,\
  /subsystem=datasources/jdbc-driver=postgresql:add(driver-name=postgresql,driver-module-name=org.postgresql.jdbc,driver-xa-datasource-class-name=org.postgresql.xa.PGXADataSource)"
----
[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_3; \
./bin/jboss-cli.sh "embed-server --server-config=standalone-ha.xml,\
  /subsystem=datasources/jdbc-driver=postgresql:add(driver-name=postgresql,driver-module-name=org.postgresql.jdbc,driver-xa-datasource-class-name=org.postgresql.xa.PGXADataSource)"
----

Finally, it is time to run the scripts for adding the PostgreSQL datasource to the {productName} servers:

[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_1; \
./bin/jboss-cli.sh -DpostgresqlUsername="test" -DpostgresqlPassword="test" \
  --file=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/extensions/postgresql-datasource.cli \
  --properties=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/extensions/cli.local.properties
----
[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_2; \
./bin/jboss-cli.sh -DpostgresqlUsername="test" -DpostgresqlPassword="test" \
  --file=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server/extensions/postgresql-datasource.cli \
  --properties=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server/extensions/cli.local.properties
----
[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_3; \
./bin/jboss-cli.sh -DpostgresqlUsername="test" -DpostgresqlPassword="test" \
  --file=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server/extensions/postgresql-datasource.cli \
  --properties=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server/extensions/cli.local.properties
----

=== Configuring EJB remoting on `server1`

EJB remote calls from `server1` to either `server2` or `server3` need to be authenticated. To achieve
this configuration, the script `${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/extensions/remoting-configuration.cli`
will be executed on `server1`.

[NOTE]
====
`remoting-configuration.cli` is configured with properties in `cli.local.properties` and runs against `standalone.xml`
====

[[remote_configuration_cli]]
[source,sh,subs="+quotes,attributes+"]
----
cd ${jbossHomeName}_1; \
./bin/jboss-cli.sh -DremoteServerUsername="quickstartUser" -DremoteServerPassword="quickstartPwd1!" \
  --file=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/extensions/remoting-configuration.cli \
  --properties=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/extensions/cli.local.properties
----

NOTE: For Windows, use the `bin\jboss-cli.bat` script.

Running `remoting-configuration.cli` results in the creation of:

* A `remote outbound socket` that points to the port on `server2`/`server3` where EJB remoting endpoints can be reached
* A https://docs.wildfly.org/22/wildscribe/subsystem/remoting/remote-outbound-connection/index.html[`remote outbound connection`] that can be referenced in the war deployment with `jboss-ejb-client.xml` descriptor
(see `${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/src/main/webapp/WEB-INF/jboss-ejb-client.xml`).
* An authentication context `auth_context` that is used by the new created remoting connection `remote-ejb-connection`; the authentication context uses the same username and password created for `server2` and `server3`

[#_start_productname_servers]
=== Start {productName} servers

At this point, the configuration of the {productName} servers is completed.
`server1` must be started with the `standalone.xml` configuration,
while `server2` and `server3` must be started with the `standalone-ha.xml` configuration to create a cluster.
As all {productName} servers will be run in the same bare metal environment,
a port offset will be applied to `server2` and `server3`. Moreover,
each server has to define a unique transaction node identifier and jboss node name.

Start each server in a separate terminal.

[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
cd ${jbossHomeName}_1; \
./bin/standalone.sh -c standalone.xml -Djboss.tx.node.id=server1 -Djboss.node.name=server1 -Dwildfly.config.url=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/configuration/custom-config.xml
----
[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
cd ${jbossHomeName}_2; \
./bin/standalone.sh -c standalone-ha.xml -Djboss.tx.node.id=server2 -Djboss.node.name=server2 -Djboss.socket.binding.port-offset=100
----
[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
cd ${jbossHomeName}_3; \
./bin/standalone.sh -c standalone-ha.xml -Djboss.tx.node.id=server3 -Djboss.node.name=server3 -Djboss.socket.binding.port-offset=200
----

NOTE: To enable the recovery of remote transaction failures, the configuration file `custom-config.xml`
should be loaded into `server1`; this operation will authenticate `server1` against `server2`/`server3`.

NOTE: For Windows, use the `bin\standalone.bat` script.

=== Deploying the Quickstart applications

. With all {productName} servers <<_setup_productname_servers, configured>> and <<_start_productname_servers, running>>,
the `client` and `server` application can be deployed

. The whole project can be built using the following commands:
+
[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/
mvn clean package
----
+
. Then, the `client` application can be deployed using the following commands:
+
[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client
mvn wildfly:deploy
----
+
. Lastly, the `server` application can be deployed using the following commands:
+
[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server
mvn wildfly:deploy -Dwildfly.port=10090
mvn wildfly:deploy -Dwildfly.port=10190
----

The commands just run employed the WildFly Maven plugin to connect to the running instances of {productName}
and deploy the `war` archives to the servers.

==== Checkpoints

. If errors occur, verify that the {productName} servers are running and that they are configured properly
. Verify that all deployments are published into all three servers
.. On `server1` check the log to confirm that the `client/target/client.war` archive is deployed
+
[source,options="nowrap"]
----
...
INFO  [org.wildfly.extension.undertow] (ServerService Thread Pool -- 76) WFLYUT0021: Registered web context: '/client' for server 'default-server'
INFO  [org.jboss.as.server] (management-handler-thread - 2) WFLYSRV0010: Deployed "client.war" (runtime-name : "client.war")
----
+
.. On `server2` and `server3`, check the log to confirm that the `server/target/server.war` archive is deployed.
+
[source,options="nowrap"]
----
...
INFO  [org.wildfly.extension.undertow] (ServerService Thread Pool -- 86) WFLYUT0021: Registered web context: '/server' for server 'default-server'
INFO  [org.jboss.as.server] (management-handler-thread - 1) WFLYSRV0010: Deployed "server.war" (runtime-name : "server.war")
----

. Verify that `server2` and `server3` formed a HA cluster.
.. Check the server log of either `server2` and `server3`, or both.
+
[source,options="nowrap"]
----
[org.infinispan.CLUSTER] () ISPN000094: Received new cluster view for channel ejb: [server2|1] (2) [server2, server3]
[org.infinispan.CLUSTER] () ISPN100000: Node server3 joined the cluster
...
INFO  [org.infinispan.CLUSTER] () [Context=server.war/infinispan] ISPN100010: Finished rebalance with members [server2, server3], topology id 5
----

[#_examining_the_quickstart]
=== Examining the Quickstart

Once the {productName} servers are configured and started, and the quickstart artifacts are deployed, it is possible to
invoke the endpoints of `server1`, which generate EJB remote invocations against the HA cluster formed by `server2` and `server3`.

The following table defines the available endpoints, and their expected behaviour.

[NOTE]
====
The endpoints return data in JSON format. You can use `curl` for the invocation
and `jq` for formatting the results. For example:

`curl -s http://localhost:8080/client/remote-outbound-stateless | jq .`
====

[NOTE]
====
On Windows, `curl` and `jq` might not be available.
If so, enter the endpoints directly to a browser of your choice.
The behaviour and the obtained JSON will be the same as for the `curl` command.
====

The HTTP invocations return the hostnames of the contacted servers.

[[rest-endpoints]]
[options="headers"]
.HTTP endpoints of the test invocation
|===
|URL |Behaviour |Expectation

|__http://localhost:8080/client/remote-outbound-stateless__
|Two invocations under the transaction context started on `server1` (`client` application).
The EJB remote call is configured from the `remote-outbound-connection`.
Both calls are directed to the same remote server instance (`server` application)
due to transaction affinity.
|The two returned hostnames must be the same.

|__http://localhost:8080/client/remote-outbound-notx-stateless__
|Several remote invocations to stateless EJB without a transaction context.
The EJB remote call is configured from the `remote-outbound-connection`.
The EJB client is expected to load balance the calls on various servers.
|The list of the returned hostnames should contain occurrences of both
 `server2` and `server3`.

|__http://localhost:8080/client/direct-stateless__
|Two invocations under the transaction context started on `server1` (`client` application). The stateless bean is invoked on the remote side.
The EJB remote call is configured from data in the `client` application source code.
The remote invocation is run via the EJB remoting protocol.
|The returned hostnames must be the same.

|__http://localhost:8080/client/direct-stateless-http__
|Two invocations under the transaction context started on `server1` (`client` application). The stateless bean is invoked on the remote side.
The EJB remote call is configured from data in the `client` application source code.
The remote invocation is run, unlike the other calls of this quickstarts, via https://docs.wildfly.org/22/Developer_Guide.html#EJB_over_HTTP[EJB over HTTP].
|The returned hostnames must be the same.

|__http://localhost:8080/client/remote-outbound-notx-stateful__
|Two invocations under the transaction context started on `server1` (`client` application).
The EJB remote call is configured from the `remote-outbound-connection`.
Both calls are directed to the same stateful bean on the remote server because
the stateful bean invocations are sticky ensuring affinity to the same server instance.
|The returned hostnames must be the same.

|__http://localhost:8080/client/remote-outbound-fail-stateless__
|An invocation under the transaction context started on `server1` (`client` application).
The call goes to one of the remote servers, where errors occur during transaction processing.
The failure is simulated at time of two-phase commit.
This HTTP call finishes with success. Only the server log shows some warnings.
This is an expected behaviour. An intermittent failure during commit phase
of two-phase protocol makes the transaction manager obliged to finish the work
eventually. The finalization of work is done in the background
(by Narayana recovery manager, see details <<_details_on_recovery, below>>), and the HTTP call may inform the client back with success.
|When the recovery manager finishes the work all the transaction resources are committed.

|===

[[_details_on_recovery]]
==== Observing the recovery processing after __client/remote-outbound-fail-stateless__ call

The EJB call to the endpoint `client/remote-outbound-fail-stateless` simulates the presence
of an intermittent network error happening at the commit phase of the two-phase commit protocol (2PC).

The http://jbossts.blogspot.com/2018/01/narayana-periodic-recovery-of-xa.html[transaction recovery manager]
periodically tries to recover the unfinished work and only when this attempt is successful,
the transaction is completed (which makes the update in the database visible). It is possible to confirm the completion of
the transaction by invoking the REST endpoint `server/commits` at both servers `server2` and `server3`.


[source]
----
curl -s http://localhost:8180/server/commits
----

[source]
----
curl -s http://localhost:8280/server/commits
----

The response of `server/commits` is a tuple composed by the host's info and the number of commits.
For example the output could be `["host: mydev.narayana.io/192.168.0.1, jboss node name: server2","3"]`
and it says that the hostname is `mydev.narayana.io`, the jboss node name is `server2`,
and the number of commits is `3`.

The Transaction recovery manager runs periodically (by default, it runs every 2 minutes) on all servers.
Nevertheless, as the transaction is initiated on `server1`, the recovery manager on this server will be
responsible to initiate the recovery process.

[NOTE]
====
The recovery process can be started manually. Using `telnet` and connecting to `localhost:4712`
(i.e. the port where https://docs.wildfly.org/22/wildscribe/subsystem/transactions/index.html#attr-recovery-listener[the recovery process is listening to]),
it is possible to send the `SCAN` command to force a recovery cycle

[source]
----
telnet localhost 4712
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
SCAN
DONE
Connection closed by foreign host.
----
====

[[_steps_to_observe_recovery_processing]]
===== Steps to observe that the recovery processing was done

. Before invoking the __remote-outbound-fail-stateless__ endpoint, double check
  the number of `commits` on `server2` and `server3` by invoking the `server/commits`
  endpoints.
+
[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
curl http://localhost:8180/server/commits; echo
# output example:
# ["host: mydev.narayana.io/192.168.0.1, jboss node name: server2","1"]
curl http://localhost:8280/server/commits; echo
# output example:
# ["host: mydev.narayana.io/192.168.0.1, jboss node name: server3","2"]
----
+
. Invoke the REST endpoint `client/remote-outbound-fail-stateless`
+
[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
curl http://localhost:8080/client/remote-outbound-fail-stateless | jq .
----
+
The JSON output from the previous command reports the name of server the request was sent to.
+

. At the server reported by the previous command, verify the number of `commits` by invoking the `server/commits` endpoint.

. Check the log of `server1` for the following warning message
+
[source,options="nowrap"]
----
ARJUNA016036: commit on < formatId=131077, gtrid_length=35, bqual_length=36, tx_uid=..., node_name=server1, branch_uid=..., subordinatenodename=null, eis_name=unknown eis name > (Subordinate XAResource at remote+http://localhost:8180) failed with exception $XAException.XA_RETRY: javax.transaction.xa.XAException: WFTXN0029: The peer threw an XA exception
----
+
This message means that the transaction manager was not able to commit the transaction as
an error occurred while committing the transaction on the remote server.
The `XAException.XA_RETRY` exception, meaning an intermittent failure, was reported in the logs.
. The logs on `server2` or `server3` contain a warning about the `XAResource` failure as well.
+
[source,options="nowrap"]
----
ARJUNA016036: commit on < formatId=131077, gtrid_length=35, bqual_length=43, tx_uid=..., node_name=server1, branch_uid=..., subordinatenodename=server2, eis_name=unknown eis name > (org.jboss.as.quickstarts.ejb.mock.MockXAResource@731ae22) failed with exception $XAException.XAER_RMFAIL: javax.transaction.xa.XAException
----
+
. Wait for the recovery process at `server1` to recover the unfinished transaction (or force a recovery cycle manually)

. The number of commits on the targeted server should be incremented by one.

include::../shared-doc/undeploy-the-quickstart.adoc[leveloffset=+1]

Repeat the last step for `server2` and `server3`:

[source,sh,options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server;
mvn wildfly:undeploy -Dwildfly.port=10090;
mvn wildfly:undeploy -Dwildfly.port=10190
----

=== Server Log: Expected Warnings and Errors

This quickstart is not production grade. The server logs include the following
warnings during the startup. It is safe to ignore these warnings.

[source,options="nowrap"]
----
WFLYDM0111: Keystore standalone/configuration/application.keystore not found, it will be auto generated on first use with a self signed certificate for host localhost

WFLYELY01084: KeyStore .../standalone/configuration/application.keystore not found, it will be auto generated on first use with a self-signed certificate for host localhost

WFLYSRV0018: Deployment "deployment.server.war" is using a private module ("org.jboss.jts") which may be changed or removed in future versions without notice.
----

// Build and run sections for other environments/builds
ifndef::ProductRelease,EAPXPRelease[]

[[build_and_run_the_quickstart_with_provisioned_server]]
== Building and running the quickstart application with provisioned {productName} server

Instead of using a standard {productName} server distribution, the three {productName} servers to deploy and run the quickstart can be alternatively provisioned by activating the Maven profile named `provisioned-server` when building the quickstart:

[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client;
mvn clean package -Pprovisioned-server \
  -DremoteServerUsername="quickstartUser" -DremoteServerPassword="quickstartPwd1!" \
  -DpostgresqlUsername="test" -DpostgresqlPassword="test"
----
[source,sh,subs="+quotes,attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server;
mvn clean package -Pprovisioned-server \
  -Dwildfly.provisioning.dir=server2 -Djboss-as.home=target/server2 \
  -DpostgresqlUsername="test" -DpostgresqlPassword="test";
mvn package -Pprovisioned-server \
  -Dwildfly.provisioning.dir=server3 -Djboss-as.home=target/server3 \
  -DpostgresqlUsername="test" -DpostgresqlPassword="test"
----

The provisioned {productName} servers, with the quickstart deployed, can then be found in the `target/server` directory, and their usage is similar to a standard server distribution, with the simplification that there is never the need to specify the server configuration to be started.

The server provisioning functionality is provided by the WildFly Maven Plugin, and you may find its configuration in the pom.xml files of the quickstart.

The quickstart user should be added before running the provisioned server:
[source,subs="+quotes,attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server;
./target/server2/bin/add-user.sh -a -u 'quickstartUser' -p 'quickstartPwd1!';
./target/server3/bin/add-user.sh -a -u 'quickstartUser' -p 'quickstartPwd1!'
----
[NOTE]
====
For Windows, use the `__{jbossHomeName}__\bin\add-user.bat` script.
====

=== Run the Integration Tests with a provisioned server

The integration tests included with this quickstart, which verify that the quickstart runs correctly, may also be run with provisioned server.

Follow these steps to run the integration tests.

. As this quickstart performs transactional work against a database, it is needed to create a new database. For the purpose of this quickstart, a simple PostgreSQL container will be used:
+
[source,sh]
----
podman run -p 5432:5432 --rm  -ePOSTGRES_DB=test -ePOSTGRES_USER=test -ePOSTGRES_PASSWORD=test postgres:9.4 -c max-prepared-transactions=110 -c log-statement=all
----

. Make sure the servers are provisioned by running the commands reported in <<build_and_run_the_quickstart_with_provisioned_server>>

. Add the quickstart user to the provisioned `server2` and `server3` by running the commands reported in <<build_and_run_the_quickstart_with_provisioned_server>>

. Start the {productName} provisioned servers in three distinct terminals, this time using the {productName} Maven Plugin, which is recommended for testing due to simpler automation.
+
[source,subs="attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client;
mvn wildfly:start -Djboss-as.home=target/server \
  -Dwildfly.javaOpts="-Djboss.tx.node.id=server1 -Djboss.node.name=server1"
----
+
[source,subs="attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server;
mvn wildfly:start -Djboss-as.home=target/server2 \
  -Dwildfly.port=10090 \
  -Dwildfly.serverConfig=standalone-ha.xml \
  -Dwildfly.javaOpts="-Djboss.socket.binding.port-offset=100 -Djboss.tx.node.id=server2 -Djboss.node.name=server2"
----
+
[source,subs="attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server;
mvn wildfly:start -Djboss-as.home=target/server3 \
  -Dwildfly.port=10190 \
  -Dwildfly.serverConfig=standalone-ha.xml \
  -Dwildfly.javaOpts="-Djboss.tx.node.id=server3 -Djboss.node.name=server3 -Djboss.socket.binding.port-offset=200"
----

. Type the following command to run the `verify` goal with the `integration-testing` profile activated, and specifying the quickstart's URL using the `server.host` system property.
+
[source,subs="attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client;
mvn verify -Pintegration-testing
----
+
[source,subs="attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server;
mvn verify -Pintegration-testing -Dserver.host="http://localhost:8180/server"
----
+
[source,subs="attributes+",options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server;
mvn verify -Pintegration-testing -Dserver.host="http://localhost:8280/server"
----

. To shut down the {productName} provisioned servers using the {productName} Maven Plugin:
+
[source,subs="attributes+",options="nowrap"]
----
mvn wildfly:shutdown
----
+
[source,subs="attributes+",options="nowrap"]
----
mvn wildfly:shutdown -Dwildfly.port=10090
----
+
[source,subs="attributes+",options="nowrap"]
----
mvn wildfly:shutdown -Dwildfly.port=10190
----

endif::[]

== Running on OpenShift

=== OpenShift deployment

Before deploying this quickstart to OpenShift Container Platform we need to inspect
a bit the platform. The {productName} server runs in a pod. The pod is an ephemeral object
that could be rescheduled, restarted or moved to a different machine by the platform.
The ephemeral nature of the pod is a poor match for the transaction manager handling transactions
and for EJB remoting passing the context as well.
The transaction manager requires a persistent log to be saved
for each {productName} server instance.
The EJB remoting requires a stable remote endpoint for connection to guarantee
the state of stateful beans and the transaction affinity.
The stability of the remote endpoint address is important for transaction
recovery calls to eventually finish the transactions too.
For these properties being guaranteed from the platform
we use _StatefulSet_ object in OpenShift.

The recommended way to deploy applications to {productName} is using
the {productName} Operator which utilizes the StatefulSet to manage {productName}
as the default option in the background.

NOTE: link:../shared-doc/cd-openshift-getting-started.adoc[Other quickstarts discuss deploying]
      of the applications with _ReplicaSet_ defined by DeploymentConfig in a template.
      It's a different approach that does not match well with the transaction affinity
      and cannot be used here.

=== Running on OpenShift: Prerequisites

include::../shared-doc/cd-create-project.adoc[leveloffset=+4]

[#_running_on_openshift_start_postgresql_database]
==== Running on OpenShift: Start PostgreSQL database

The quickstart requires the PostgreSQL database to be running.
For testing purposes you can use the provided yaml template
which deploys the database on your OpenShift instance
(usable only for the testing purpose).

[source,sh,options="nowrap",subs="+quotes,attributes+"]
----
# change path to ${PATH_TO_QUICKSTART_DIR}
cd ${PATH_TO_QUICKSTART_DIR}
# deploy not-production ready XA capable PostgreSQL database to OpenShift
oc new-app --namespace=$(oc project -q) \
           --file=${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/extensions/postgresql.deployment.yaml
----

[#_running_on_openshift_build_the_application]
==== Running on OpenShift: Build the application

For building the application the {productName} s2i functionality
(provided out-of-boxy by OpenShift) needs to be used.

The {productName} provides `ImageStream` definition of builder images and runtime images.
The builder image makes the application to be built, configure the application server
with s2i scripts. The resulted image may be used for running,
but it's big as it contains many dependencies needed only for the build.
The chain build defines the next step which is to get the runtime image
and copy there the configured {productName} server with the application.

ifndef::ProductRelease,EAPCDRelease,EAPXPRelease[]
[source,sh,options="nowrap",subs="+quotes,attributes+"]
----
# Install builder image streams
oc create -f {ImageandTemplateImportURL}imagestreams/wildfly-centos7.json
# Install runtime image streams
oc create -f {ImageandTemplateImportURL}imagestreams/wildfly-runtime-centos7.json

# Deploy template with chain build to get the quickstart
# being deployed within runtime image
oc create -f {ImageandTemplateImportURL}templates/wildfly-s2i-chained-build-template.yml
----
endif::[]

ifdef::ProductRelease,EAPCDRelease,EAPXPRelease[]
[source,sh,options="nowrap",subs="+quotes,attributes+"]
----
# Install builder and runtime image streams
oc create -f {ImageandTemplateImportURL}{ImagePrefixVersion}-openjdk11-image-stream.json

# Deploy template with chain build to get the quickstart
# being deployed within runtime image
oc create -f {ImageandTemplateImportBaseURL}/master/eap-s2i-build.yaml
----
endif::[]

The final step for starting the application start
is the `s2i` chain build execution.

ifndef::ProductRelease,EAPCDRelease,EAPXPRelease[]
[source,sh,options="nowrap",subs="+quotes,attributes+"]
----
# s2i chain build for client application
oc new-app --template=wildfly-s2i-chained-build-template \
  -p IMAGE_STREAM_NAMESPACE=$(oc project -q) \
  -p APPLICATION_NAME=client \
  -p GIT_REPO={githubRepoUrl} \
  -p GIT_BRANCH={productVersion}.0.0.Final \
  -p GIT_CONTEXT_DIR=ejb-txn-remote-call/client
# s2i chain build for server application
oc new-app --template=wildfly-s2i-chained-build-template \
  -p IMAGE_STREAM_NAMESPACE=$(oc project -q) \
  -p APPLICATION_NAME=server \
  -p GIT_REPO={githubRepoUrl} \
  -p GIT_BRANCH={productVersion}.0.0.Final \
  -p GIT_CONTEXT_DIR=ejb-txn-remote-call/server
----
endif::[]

ifdef::ProductRelease,EAPCDRelease,EAPXPRelease[]
[source,sh,options="nowrap",subs="+quotes,attributes+"]
----
# s2i chain build for client application
oc new-app --template=eap-s2i-build \
  -p EAP_IMAGESTREAM_NAMESPACE=$(oc project -q) \
  -p APPLICATION_IMAGE=client \
  -p SOURCE_REPOSITORY_URL={EAPQuickStartRepo} \
  -p SOURCE_REPOSITORY_REF={EAPQuickStartRepoRef} \
  -p CONTEXT_DIR=ejb-txn-remote-call/client \
  -p EAP_IMAGE={BuildImageStream} \
  -p EAP_RUNTIME_IMAGE={RuntimeImageStream}
# s2i chain build for server application
oc new-app --template=eap-s2i-build \
  -p EAP_IMAGESTREAM_NAMESPACE=$(oc project -q) \
  -p APPLICATION_IMAGE=server \
  -p SOURCE_REPOSITORY_URL={EAPQuickStartRepo} \
  -p SOURCE_REPOSITORY_REF={EAPQuickStartRepoRef} \
  -p CONTEXT_DIR=ejb-txn-remote-call/server \
  -p EAP_IMAGE={BuildImageStream} \
  -p EAP_RUNTIME_IMAGE={RuntimeImageStream}
----
endif::[]

Wait for the builds to finish. You can verify the build status by executing
the `oc get pod` command.

The expected output, after few whiles, shows that the `*-build` jobs
have got the value `Completed` in the `STATUS` column.

[source,sh,options="nowrap"]
----
oc get pod
NAME                                READY   STATUS      RESTARTS   AGE
client-2-build                      0/1     Completed   0          35m
client-build-artifacts-1-build      0/1     Completed   0          45m
server-2-build                      0/1     Completed   0          15m
server-build-artifacts-1-build      0/1     Completed   0          19m
----

[#_running_on_openshift_install_productname_operator]
==== Running on OpenShift: Install {productName} Operator

With the prior step we have got the application image with the server being part of.
The next step is to install the {productName} Operator which is responsible
for managing the life cycle of the application image.

ifndef::ProductRelease,EAPCDRelease,EAPXPRelease[]
Manual installation of the {productName} Operator on OpenShift is summarized in the script
https://github.com/wildfly/wildfly-operator/blob/master/build/run-openshift.sh[build/run-openshift.sh].

[source,sh]
----
git clone https://github.com/wildfly/wildfly-operator/
./wildfly-operator/build/run-openshift.sh
----
endif::[]

ifdef::ProductRelease,EAPCDRelease,EAPXPRelease[]
Consult necessary steps to install the {productName} Operator
at {productName} documentation
https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/

The following steps can be used as a quickstart guide

[source,sh,options="nowrap",subs="+quotes,attributes+"]
----
# 1. log in to account with cluster-admin permission
# 2. create a YAML file containing Subscription object
cat >> /tmp/eap-operator-sub.yaml << EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: eap
  namespace: openshift-operators
spec:
  channel: alpha
  installPlanApproval: Automatic
  name: eap
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
# 3. create the Subscription object in the OpenShift
oc apply -f eap-operator-sub.yaml
# 4. verify the openshift-operators namespace that the object was created
oc get csv -n openshift-operators
----
endif::[]

[#_running_on_openshift_run_the_quickstart_with_productname_operator]
==== Running on OpenShift: Run the Quickstart with {productName} Operator

After you install the {productName} Operator, you can deploy the `CustomResource` that uses it.
The `CustomResource.yaml` definition contains information the {productName} Operator uses
to start the application pods for the client application and for the server application.

Before deploying the application, ensure the `view` permissions for the default system account
is set up. The `KUBE_PING` protocol, that is used for forming the HA {productName} cluster
on OpenShift, requires `view` permissions to read labels of pods.

[source,sh]
----
oc policy add-role-to-user view system:serviceaccount:$(oc project -q):default -n $(oc project -q)
----

After granting the permissions, deploy the `CustomResource`, managed by {productName} Operator
that, referencing to the <<_running_on_openshift_build_the_application, built application images>>.

[CAUTION]
====
Adjust the value of the `applicationImage` value in the OpenShift deployment templates
`${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/client/client-cr.yaml` and
`${PATH_TO_QUICKSTART_DIR}/ejb-txn-remote-call/server/server-cr.yaml`
to match the project. It is the most probably {artifactId}-project.
====

[source,sh,options="nowrap"]
----
cd ${PATH_TO_QUICKSTART_DIR}

# deploying client definition, one replica, PostgreSQL database has to be available
oc create -f ejb-txn-remote-call/client/client-cr.yaml
# deploying server definition, two replicas, PostgreSQL database has to be available
oc create -f ejb-txn-remote-call/server/server-cr.yaml
----

If these commands are successful, the `oc get pod` command shows
all the pods required for the quickstart, namely the quickstart client and two
server pods and the PostgreSQL database pod that the application connects to.

[source,sh,options="nowrap"]
----
NAME                                READY   STATUS      RESTARTS   AGE
client-0                            1/1     Running     0          29m
postgresql-f9f475f87-l944r          1/1     Running     1          22h
server-0                            1/1     Running     0          11m
server-1                            1/1     Running     0          11m
----

To observe the {productName} Operator look at

ifndef::ProductRelease,EAPCDRelease,EAPXPRelease[]
[source,sh,options="nowrap",subs="+quotes,attributes+"]
----
oc get po -n $(oc project -q)

NAME                                READY   STATUS      RESTARTS   AGE
wildfly-operator-5d4b7cc868-zfxcv   1/1     Running     1          22h
----
endif::[]

ifdef::ProductRelease,EAPCDRelease,EAPXPRelease[]
[source,sh,options="nowrap",subs="+quotes,attributes+"]
----
oc get po -n openshift-operators

NAME                            READY   STATUS    RESTARTS   AGE
eap-operator-75c77c789c-2zdnd   1/1     Running   0          86m
----
endif::[]

[#_running_on_openshift_verify_the_quickstarts]
==== Running on OpenShift: Verify the Quickstarts

The {productName} Operator creates routes that make the applications accessible
outside the OpenShift environment. Run the `oc get route` command to find the location of the REST endpoint.
An example of the output is:

[source,sh,options="nowrap"]
----
NAME           HOST/PORT                                                            PATH   SERVICES              PORT
client-route   client-route-ejb-txn-remote-call-client-artifacts.apps-crc.testing          client-loadbalancer   http
server-route   server-route-ejb-txn-remote-call-client-artifacts.apps-crc.testing          server-loadbalancer   http
----

For HTTP endpoints provided by the quickstart application
check <<rest-endpoints,the table above>>.

[source,sh,options="nowrap"]
----
curl -s $(oc get route client-route --template='{{ .spec.host }}')/client/remote-outbound-stateless | jq .
curl -s $(oc get route client-route --template='{{ .spec.host }}')/client/remote-outbound-stateless | jq .
curl -s $(oc get route client-route --template='{{ .spec.host }}')/client/remote-outbound-notx-stateless | jq .
curl -s $(oc get route client-route --template='{{ .spec.host }}')/client/direct-stateless | jq .
curl -s $(oc get route client-route --template='{{ .spec.host }}')/client/remote-outbound-notx-stateful | jq .
----

If you like to <<_steps_to_observe_recovery_processing,observe the recovery processing>>
then you can follow these shell commands.

[source,sh,options="nowrap"]
----
# To check failure resolution
# verify the number of commits that come from the first and second node of the `server` deployments.
# Two calls are needed, as each reports the commit count of different node.
# Remember the reported number of commits to be compared with the results after crash later.
curl -s $(oc get route server-route --template='{{ .spec.host }}')/server/commits
curl -s $(oc get route server-route --template='{{ .spec.host }}')/server/commits

# Run the remote call that causes the JVM of the server to crash.
curl -s $(oc get route client-route --template='{{ .spec.host }}')/client/remote-outbound-fail-stateless
# The platforms restarts the server back to life.
# The following commands then make us waiting while printing the number of commits happened at the servers.
while true; do
  curl -s $(oc get route server-route --template='{{ .spec.host }}')/server/commits
  curl -s $(oc get route server-route --template='{{ .spec.host }}')/server/commits
  I=$((I+1))
  echo " <<< Round: $I >>>"
  sleep 2
done
----

==== Running on OpenShift: Quickstart application removal

To remove the application you need to remove the `WildFlyServer` definition
(which was deployed by <<_running_on_openshift_run_the_quickstart_with_productname_operator,`*-cr` yaml descriptor>>).
You can do that by running `oc remove WildFlyServer client`
and `oc remove WildFlyServer server`.
With that the application will be stopped, and the pods will be removed.

== Conclusion

This quickstarts is a showcase for the EJB remoting calls from one {productName} server
to other with transaction propagation being involved.
It shows things from multiple areas from setting-up the datasources, through security definition
for remote connection, EJB remoting in the application, up to observing the transaction recovery processing.
On top of that it shows running this all in OpenShift managed with {productName} Operator.
